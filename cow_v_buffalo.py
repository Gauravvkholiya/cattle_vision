# -*- coding: utf-8 -*-
"""cow-v-buffalo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18eUi6S-Ac653Y8IbNCjz8H6_MIGqPh9D
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout

# due to more images processing of all images is very hard so we use generators as ram kam pd jata h
# keras use generators it divide the data in batches
train_ds =keras.utils.image_dataset_from_directory(
    directory = '/content/drive/MyDrive/data/train',
    labels= 'inferred',
    label_mode = 'int',   #no to images cows 0 and buffalo 1
    batch_size = 32,
    image_size = (256,256)    #image 256*256 m resize ho jayegoi
)

validation_ds =keras.utils.image_dataset_from_directory(
    directory = '/content/drive/MyDrive/data/test',
    labels= 'inferred',
    label_mode = 'int',   #no to images cows 0 and buffalo 1
    batch_size = 32,
    image_size = (256,256)    #image 256*256 m resize ho jayegoi
)

#these files are stored in numpy array format store hua h isme har value 0 to 255 h

#normalise
def process(image,label):
  image = tf.cast(image/255. ,tf.float32)
  return image,label

train_ds = train_ds.map(process)
validation_ds = validation_ds.map(process)

#the above process is done because we want on value betweeen 0 and 1

#creating cnn model and passing the data
# 3 convocation layer
# 1 layer 32 filters 2 layer 64 filter and 3 layer 128 filter

model =  Sequential()

model.add(Conv2D(32,kernel_size= (3,3),padding ='valid',activation='relu',input_shape = (256,256,3)) )
# model.add(BatchNormalization())
#after convocation layer there will be the pooling layer
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(64,kernel_size= (3,3),padding ='valid',activation='relu'))
# model.add(BatchNormalization())
#after convocation layer there will be the pooling layer
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(128,kernel_size= (3,3),padding ='valid',activation='relu'))
# model.add(BatchNormalization())
#after convocation layer there will be the pooling layer
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Flatten())

model.add(Dense(128,activation='relu'))
# model.add(Dropout(0.1))
model.add(Dense(64,activation='relu'))
# model.add(Dropout(0.1))
model.add(Dense(1,activation='sigmoid'))

# model  = Sequential()
# model.add(Dense(128,input_dim = 2,activation="relu",kernel_regularizer=tf.keras.regularizers.l2(0.03)))
# model.add(Dense(128,activation="relu",kernel_regularizer=tf.keras.regularizers.l2(0.03)))
# model.add(Dense(1,activation='sigmoid'))

model.summary()

#compiling model

model.compile(optimizer ='adam',loss='binary_crossentropy',metrics =['accuracy'])

#running the model
history = model.fit(train_ds,epochs=10,validation_data= validation_ds)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'],color='red',label='train')
plt.plot(history.history['val_accuracy'],color = 'blue',label ='validation')
plt.legend()
plt.show()

#clearly we can see the gap between train and validation
#now plotting the validation loss and train loss graph
plt.plot(history.history['loss'],color='red',label='train')
plt.plot(history.history['val_loss'],color = 'blue',label ='validation')
plt.legend()
plt.show()

#after using the batchnormalization and dropout

plt.plot(history.history['loss'],color='red',label='train')
plt.plot(history.history['val_loss'],color = 'blue',label ='validation')
plt.legend()
plt.show()

import cv2

test_img = cv2.imread('/content/cow.jpg')

plt.imshow(test_img)

test_img.shape

test_img = cv2.resize(test_img,(256,256))

test_input = test_img.reshape((1,256,256,3)) #we have tell we are having only one image in this btch

model.predict(test_input)